\section{Algorithm IROC}
%The pivotal procedural of detecting overlapping community is how to assign vertex to multiple community. In this paper, we achieve vertex overlapping by constructing redundancy from small subgraphs.
\noindent In this section, we propose an effective and efficient searching algorithm IROC to detect overlapping clusters in an attributed graph. The initialization phase and the refinement phase are the two key steps in IROC. The initialization phase is again divided into two subroutines for a) creating graph substructures and b) finding their coherent attribute subspace. The refinement phase takes the responsibility of improving the quality of initial clusters by a) removing redundant parts of the cluster and b) reassigning vertices between two clusters. 

\subsection{Initialization}
\noindent \textbf{Creating Graph Substructures}: The idea for the implicit creation of an overlapping graph structure is to create small redundant subgraphs. Each small subgraphs is simply composed of a vertex and all its neighbors. This is an intuitive way to obtain all these small redundant subgraphs. Therefore, $N$ subgraphs with overlapping vertices are extracted from an attributed graph $G$, where $N$ is the number of vertices in $G$. First of all, in order to eliminate over much redundancies we select $K_s$ subgraphs as initial rough clusters based on the MDL principle automatically. The procedure is shown in Algorithm \ref{alg:Initialization}. To be specific, we consider $N$ subgraphs as rough clusters. And we consider there is no clusters in the attributed graph $G$. Then we add a rough cluster to the clusters $C$ and calculate the coding cost by Eq.(\ref{eq:mdl}) one by one. After that we select the cluster with the minimum coding cost as the first cluster. We keep the selected cluster in the clusters $C$. Analogously, in the remaining $N-1$ candidates, we try to add each candidate as the second cluster and choose the one with the minimum coding cost repetitively. This process stops until all vertices are processed, so that $K_s$ initial clusters are chosen automatically.
\\

\noindent \textbf{Finding a Coherent Attribute Subspace}: After obtaining $K_s$ rough clusters, we also need to find out the attribute subspace of each rough clusters. Algorithm \ref{alg:findsubspace} demonstrates the progress of searching an attribute subspace of a cluster $C_i$ based on the MDL principle. Every vertex of a rough cluster possesses a category in each attribute. For each cluster, we calculate the entropy of each attribute to measure the purity of the categories in the attribute. Entropy in this case refers to the consistence of categories of an attribute. The lower the value, the higher the consistency of the attribute. Thus we order the attributes in ascending order of their entropies. If some attributes possesses the same value, they are formed into groups and considered as one candidate. Thus the candidates of attributes are form as $\Lambda^o=\{\Lambda^o_1,\Lambda^o_2,...,\Lambda^o_{T_g} \}$. Based on the prerequisite that the subspace of the cluster is initialized with an empty set, we add the ordered attribute candidates to the subspace one by one, and meanwhile calculate the coding cost of the whole graph $G$ by Eq.(\ref{eq:mdl}). Then we select the attribute subspace $\Lambda'$ of a cluster $C_i$ with the minimum coding cost. The attribute subspace of the cluster is fixed after chosen. In the same way, the attribute subspace of other $K_s-1$ clusters are chosen.   

\begin{algorithm}[htbp]
\caption{\textbf{Creating Subgraphs}}
\label{alg:Initialization}
\begin{algorithmic}[1]
\REQUIRE
    Attributed Graph $G$ with $N$ Vertices and $T$ Attributes 
\ENSURE
    $K_s$ Rough Clusters: $C =\{C_1,C_2,...,C_{K_s}\}$
\STATE Construct $N$ subgraphs $SS = \{ss_1,ss_2,...,ss_N \}$ as initial $N$ clusters;
\STATE $C \leftarrow \emptyset$;
\WHILE{All vertices are ergodic}
\FOR{All substructures in $SS$}
\STATE Calculate coding cost of $ss_i \bigcup C$;
\ENDFOR
\STATE Select $ss_m$ with minimum coding cost, $ss_m \bigcup C$;
\STATE Remove $ss_m$ from $SS$;
\ENDWHILE
\RETURN {$K_s$ Rough Clusters: $C =\{C_1,C_2,...,C_{K_s}\}$}.
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[htbp]
\caption{\textbf{Finding Subspace}}
\label{alg:findsubspace}
\begin{algorithmic}[1]
\REQUIRE
    A Cluster $C_i$ with $N_c$ Vertices and $T$ Attributes
\ENSURE
    Subspace $\Lambda' = \{\lambda'_1,\lambda'_2,...,\lambda'_S \}$

\STATE $\Lambda'\leftarrow\emptyset$;
\FOR{$i$ from $1$ to $T$}
\STATE Calculate entropy of each attribute $\lambda_i$;
\ENDFOR
\STATE Group attributes with same entropy and arrange attributes in ascending order $\Lambda^o=\{\Lambda^o_1,\Lambda^o_2,...,\Lambda^o_{T_g} \}$;
\FOR{$i$ form $1$ to $T_g$}
\STATE $\Lambda^o_i\bigcup \Lambda'$, and calculate coding cost of all clusters $CC$.
\IF {$CC$ increases}
\STATE break;
\ENDIF
\ENDFOR
\RETURN {Subspace $\Lambda' = \{\lambda'_1,\lambda'_2,...,\lambda'_S \}$}.
\end{algorithmic}
\end{algorithm}

\subsection{Refinement}
\noindent \textbf{Removing Redundancy:} After the initialization step, we receive $K_s$ small attributed clusters which contain \emph{redundant} information. These redundancies serve to generate and find the overlapping in the beginning but are later unwelcome. Therefore our heuristic bottom up algorithm needs to merge these small initial rough clusters to remove redundancies. We define an information-theoretic based similarity which measures the degree of the redundancy between every pair of clusters. Due to the fact that both structural and attribute information are present, we measure the similarity of two clusters as shown in Eq. (\ref{eq:similarity}):
\begin{equation}
Sim(C_i,C_j) = H(O_A)+ H(O_F).
\label{eq:similarity}
\end{equation}
where $H(\cdot)$ is entropy defined in Eq. (\ref{eq:entropy}), $O_A$ and $O_F$ are the structural and attribute overlapping part of cluster $C_i$ and $C_j$ respectively. This quality function tackles the chief problem of merging two cluster: the amount of overlaps in structural and attribute information. How can two cluster be merged without creating new redundancy if they are overlapping in structure and attribute space? On the structural aspect, entropy paysoff the denser the overlapping area. Also the smaller the entropy, the denser the overlapping part of the cluster. Thus the smaller entropy of structural overlapping part leads us to detect dense clusters. While on the attribute side, the entropy measures the purity of categories in the overlapping area of the attributes. This ensures that the cluster is provided with same categories in the attributes. The smaller the entropy is, the higher the similarity of the attributes in the overlapping part is. Thus the smaller entropy of  attribute overlapping part leads us to detect clusters with coherent meaning. Therefore, the defined similarity balances the entropy of the two aspects, which guides us to merge the two most similar clusters. And in every search run we merge the pair of clusters that have a minimal similarity.
\\

\noindent \textbf{Assigning Vertices:} Obviously, merging two clusters $C_i$ and $C_j$ to form a new cluster $C_{new}$ is able to remove redundancy. However, such merging may not eliminate all redundancies. Depending on how accurate the removing of the redundancy works, we need to modify the cluster further in a second refinement step which is shown in Algorithm \ref{alg:av}. For all vertices $\{v_1,v_2,...,v_{N_{C_{new}}}\}$ in $C_{new}$, we try to split a vertex $v_1$ from $C_{new}$ and consider it as a new cluster $C_{split}=\{v_1\}$. Then we find the subspace of $C_{split}$ and refine the subspace of $C_{new}=\{v_2,...,v_{N_{C_{new}}}\}$. If such process reduce the coding cost, we keep the modification and try to move the next vertex $v_2$ from $C_{new}$ to $C_{split}$. If the coding cost is not reduced, we move the vertex $v_1$ back to $C_{new}$. The refinement of cluster $C_{new}$ ends when the coding cost achieves its local minimum. If cluster $C_{split}$ is not empty, we treat $C_{split}$ as a new candidate and add it to the clusters. 

\begin{algorithm}[htbp]
\caption{\textbf{Assigning Vertices}}
\label{alg:av}
\begin{algorithmic}[1]
\REQUIRE
    Cluster $C_{new}$
\ENSURE
    Cluster $C_{new}$ and Cluster $C_{split}$
\STATE Creat a cluster $C_{split}$, $C_{split} \leftarrow \emptyset$;
\STATE Calculate coding cost of all the clusters $CC$;
\FOR{All nodes in $C_{new}$}
\STATE $v \in C_{new}$, remove $v$ from $C_{new}$, add $v$ to $C_{split}$;
\STATE \textbf{Finding Subspace} of $C_{split}$ and $C_{new}$;
\STATE Calculate coding cost of all the clusters $CC_{new}$
\IF{$CC_{new}$ $>$ $CC$}
\STATE Restore $v$ to $C_{new}$;
\ENDIF
\ENDFOR
\RETURN {$C_{new}$ and $C_{split}$}.
\end{algorithmic}
\end{algorithm}
 
\subsection{IROC}
\noindent The overall procedure of IROC is shown in Algorithm \ref{alg:GraphClustering}. First, we automatically select $K_s$ rough clusters and search their attribute subspace as described in the initialization phase. Then we calculate the similarity of every pair of clusters, and merge the two cluster with a minimum similarity. After that a new cluster $C_{new}$ is formed and we find the subspace of it. Then we try to assign vertices from $C_{new}$ to $C_{split}$ under the control of MDL. And we consider $C_{split}$ as a new cluster and recalculate the similarity of every pair of clusters. The merging process continues iteratively and is ended when the coding cost of all clusters achieve its local minimum. Finally, $K$ clusters with coherent attribute subspaces without redundancy are output.

\begin{algorithm}[htbp]
\caption{\textbf{IROC}}
\label{alg:GraphClustering}
\begin{algorithmic}[1]
\REQUIRE
    Attributed Graph $G$
\ENSURE
    $K$ Clusters with Subspace $C = {C_1, C_2,...,C_K}$
\STATE \textbf{Creating Subgraphs} $C = {C_1, C_2,...,C_{K_s}}$ ;
\FOR{$i$ from 1 to $K_s$}
\STATE \textbf{Finding Subspace} of $C_i$
\ENDFOR
\WHILE {Converge}
\STATE Calculate similarity of every pair of clusters;
\STATE Merge two most similar clusters as $C_{new}$;
\STATE \textbf{Finding Subspace} of $C_{new}$;
\STATE \textbf{Assigning Vertices} of $C_{new}$;
\ENDWHILE
\RETURN {$C = {C_1, C_2,...,C_K}$}.
\end{algorithmic}
\end{algorithm}

\subsection{Complexity Analysis}
\noindent Suppose we have an attributed graph $G$ with $N$ vertices, $E$ edges and $T$ attributes for each vertex. We first analyse the complexity of the coding when we have $K$ clusters. For each clusters we need to count the edges of the cluster and go through its vertices and attributes to get the probability distributions. The complexity of these processes for all clusters are $O((K+1)\cdot(A_v{_E}+A_v{_N}\cdot A_v{_T}))$, where $A_v{_E} < E$ is the average number of edges, $A_v{_N} < N$ is the average number of vertices and $A_v{_T} < T$ is the average number of subspace attributes. The multiplier factor is $K+1$ considering the no-cluster area. Since the number of cluster is $K \ll N$, the complexity of the proposed coding scheme is $O(E+N\cdot T)$.

In initialization phase of IROC, we greedily choose the initial clusters, since their attributes are not considered in this part, the complexities are $O(K_s\cdot N)$ and $O(N\cdot E)$ respectively, where $K_s$ is the number of initial clusters. Then we need to find the subspace for each cluster. In this part only attributes are considered, its complexity is $O(K_s \cdot N\cdot T^2)$. The reason is that for each cluster we need to calculate the coding cost $T$ times. Finally, the complexity of the full initialization phase (both parts combined) is $O(K_s(N + N\cdot T^2))$ for the random one and $O(K_s\cdot N(E+T^2))$ for the greedy approach.

In the full refinement step, we need to process $K_s^2$ pairs of clusters. In each merging and splitting process, we need to move vertices to other clusters and then calculate the subspace of the new clusters. Therefore, we need $O(N^2\cdot T^2)$ time to do so. Finally, the complexity in this step is $K_s^2\cdot O(N^2\cdot T^2)$.



